{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa962b12",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "`pip install onnx onnxruntime onnxruntime-gpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b79e9ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "INFERENCE_BATCH_SIZE=64\n",
    "TRAIN_BATCH_SIZE=128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffea08e4",
   "metadata": {},
   "source": [
    "## 1. Train a resnet18 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5c7c7f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "train_dataset = CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=TRAIN_BATCH_SIZE,\n",
    "                          shuffle=True, num_workers=1)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=INFERENCE_BATCH_SIZE,\n",
    "                         shuffle=False, num_workers=1, drop_last=True)\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6326f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, lr, writer, start=0, test_every=5):\n",
    "\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
    "    \n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images.to(device))\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        \n",
    "#         writer.add_scalar(\"Training loss\", avg_loss, start+epoch+1)\n",
    "        print(f'Epoch {start + epoch + 1}/{start + num_epochs}, Loss: {avg_loss:.3f}, LR: {scheduler.get_last_lr()[0]}')\n",
    "\n",
    "        if (start+epoch+1) % test_every == 0:\n",
    "            acc, inf_time = evaluate(model)\n",
    "#             writer.add_scalar(\"Test acc\", acc, start+epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "237c2534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, half_precision=False):\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    inf_time = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            \n",
    "            if half_precision:\n",
    "                images = images.half()\n",
    "\n",
    "            start = time.time()    \n",
    "            outputs = model(images)\n",
    "            inf_time += (time.time() - start)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total * 100\n",
    "    inf_time = inf_time / (len(test_loader) * INFERENCE_BATCH_SIZE) * 1_000\n",
    "    \n",
    "    print(f'Test Accuracy: {accuracy:.2f}%, Inference time per sample: {inf_time:.3f} ms (inference batch size: {INFERENCE_BATCH_SIZE})')\n",
    "\n",
    "    return accuracy, inf_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5df01ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4359c780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001\n",
      "Epoch 1/10, Loss: 1.430, LR: 2.4471741852423235e-05\n",
      "Epoch 2/10, Loss: 1.053, LR: 0.0009045084971874203\n",
      "Epoch 3/10, Loss: 0.876, LR: 0.00020610737385379736\n",
      "Epoch 4/10, Loss: 0.749, LR: 0.0006545084971873971\n",
      "Epoch 5/10, Loss: 0.643, LR: 0.0005000000000002132\n",
      "Test Accuracy: 71.73%, Inference time per sample: 0.058 ms (inference batch size: 64)\n",
      "Epoch 6/10, Loss: 0.757, LR: 0.0003454915028125687\n",
      "Epoch 7/10, Loss: 0.573, LR: 0.0007938926261462524\n",
      "Epoch 8/10, Loss: 0.478, LR: 9.549150281258283e-05\n",
      "Epoch 9/10, Loss: 0.405, LR: 0.0009755282581478662\n",
      "Epoch 10/10, Loss: 0.348, LR: 0.0\n",
      "Test Accuracy: 73.08%, Inference time per sample: 0.061 ms (inference batch size: 64)\n"
     ]
    }
   ],
   "source": [
    "model = resnet18(num_classes=10).to(device)\n",
    "lr = 1e-3\n",
    "\n",
    "train(model, num_epochs=10, lr=lr, writer=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42dd597",
   "metadata": {},
   "source": [
    "## 2. Convert PyTorch model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "39621fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.rand(1, 3, 32, 32).to(device)\n",
    "\n",
    "## NEVERMIND the following. The forward function implementation is not simple so\n",
    "# instead of returning a ScriptModule, trace() is returning a TopLevelTracedModule \n",
    "# which indicates that the model was not entirely traceable.\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# convert the model to a ScriptModule using a dummy input tensor\n",
    "model_sm = torch.jit.trace(model, example_inputs=dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1c5b8cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.models.resnet.ResNet'>\n",
      "<class 'torch.jit._trace.TopLevelTracedModule'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))\n",
    "print(type(model_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4c3ec3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the model to an ONNX model using torch.onnx.export\n",
    "\n",
    "# make sure the model is in eval mode so that layers that \n",
    "# behave differently start doing so before tracing begins\n",
    "model.eval()\n",
    "\n",
    "torch.onnx.export(model,\n",
    "                  args=dummy_input,                 # won't be used since we're passing a ScriptModule\n",
    "                  f=\"fp32.onnx\",\n",
    "                  opset_version=17,\n",
    "                  input_names = ['input'],\n",
    "                  output_names = ['output'],\n",
    "                  dynamic_axes={\n",
    "                      'input' : {0 : 'batch_size'},    # flexibility for batch dim\n",
    "                      'output' : {0 : 'batch_size'}\n",
    "                      }\n",
    "                 )\n",
    "\n",
    "# Running this means we are done with PyTorch. The model is now in ONNX \n",
    "# so we'll read it in using Onnx and move forward from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081690b4",
   "metadata": {},
   "source": [
    "## 3. Run the ONNX model using ONNX runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "735d405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a75d428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## With the release of ONNX runtime, the following isn't \n",
    "# needed. We can directly pass the model file to the session \n",
    "# creator. \n",
    "\n",
    "# model_onnx = onnx.load(\"fp32.onnx\")\n",
    "# onnx.checker.check_model(model_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e0a5757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an onnx runtime session\n",
    "options = onnxruntime.SessionOptions()\n",
    "options.enable_profiling=True\n",
    "session = onnxruntime.InferenceSession(\"fp32.onnx\",   # alternatively, can pass model_onnx \n",
    "                                      providers=[\n",
    "                                          'CUDAExecutionProvider',\n",
    "                                          'CPUExecutionProvider',\n",
    "#                                           'TensorrtExecutionProvider',\n",
    "                                      ],\n",
    "                                      sess_options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6b383f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "# the exection providers (EPs) list is traversed in order\n",
    "# if a given EP is available, it's used, otherwise we move \n",
    "# on to the next one. Since CUDA support was available while starting the\n",
    "# InferenceSession, the following list had returned both the EPs.\n",
    "# If there was no CUDA support, the list would have only CPUExecutionProvider\n",
    "# despite passing other EPs in the InferenceSession initialization\n",
    "print(session.get_providers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "686f9336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(x):\n",
    "    return x.detach().cpu().numpy() if x.requires_grad else x.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b19d07a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input dictionary with {name: tensor} key,value pairs. \n",
    "x = torch.randn(8, 3, 32, 32).to(device)\n",
    "inputs = {session.get_inputs()[0].name: to_numpy(x)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "604f2813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 65.4 ms, sys: 84.1 ms, total: 150 ms\n",
      "Wall time: 263 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'onnxruntime_profile__2024-03-01_03-59-18.json'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# The `output_names` argument is used for output selection.\n",
    "# Passing None means no selection, return everything\n",
    "output = session.run(None, inputs)\n",
    "session.end_profiling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b4f14fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 10)\n"
     ]
    }
   ],
   "source": [
    "print((output[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0b87eac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['TensorrtExecutionProvider',\n",
       " 'CUDAExecutionProvider',\n",
       " 'AzureExecutionProvider',\n",
       " 'CPUExecutionProvider']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(onnxruntime.get_device())\n",
    "onnxruntime.get_available_providers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad8ec18",
   "metadata": {},
   "source": [
    "## 4. Quantize the ONNX model (CPU)\n",
    "\n",
    "As per `onnxruntime` documentation [here](https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html), by quantization they mean 8-bit linear quantization of an ONNX model.  \n",
    "There are two things that need to be done before running `quantize_static()`:\n",
    "1. First, we need to \"preprocess\" the model using the preprocess script provided by ONNX. \n",
    "2. We need to write a CalibrationDataReader that would provide data for the quantization calibration process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4019d7b",
   "metadata": {},
   "source": [
    "### 4.1 Preprocess the model\n",
    "\n",
    "This runs shape inference for the model and performs optimizations in the computation graph like fusing operations for efficiency (e.g., conv+bn), reducing redundancy, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2facd150",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m onnxruntime.quantization.preprocess --input fp32.onnx --output fp32_preproc.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edcce50",
   "metadata": {},
   "source": [
    "### 4.2 Quantize the model (including calibration)\n",
    "\n",
    "Define a `CalibrationDataReader` object to provide the calibration data needed for static quatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "904c732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_calib_batches = 64\n",
    "CALIB_BATCH_SIZE = 8\n",
    "\n",
    "# which samples to use from the training dataset for calibration\n",
    "indices = list(np.random.choice(len(test_dataset), size=num_calib_batches*CALIB_BATCH_SIZE, replace=False))\n",
    "\n",
    "calib_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=CALIB_BATCH_SIZE,\n",
    "                          sampler=SubsetRandomSampler(indices), \n",
    "                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2a009ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime.quantization as oqt\n",
    "\n",
    "# create the CalibrationDataReader for your dataset\n",
    "class ResNetCalibLoader(oqt.CalibrationDataReader):\n",
    "    def __init__(self, dataloader, model_path):\n",
    "        self.dataloader = dataloader\n",
    "        self.enum_data = None\n",
    "        \n",
    "        # create an inference session to find the input name\n",
    "        session = onnxruntime.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
    "        self.input_name = session.get_inputs()[0].name\n",
    "        \n",
    "    def get_next(self):\n",
    "        if self.enum_data is None:\n",
    "            self.enum_data = iter(\n",
    "                [{self.input_name: im.cpu().numpy()} for im, _ in self.dataloader])\n",
    "        \n",
    "        # None is the value to return when the iterator is empty\n",
    "        return next(self.enum_data, None)\n",
    "        \n",
    "    def rewind(self,):\n",
    "        self.enum_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "22556686",
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_reader = ResNetCalibLoader(calib_loader, \"fp32_preproc.onnx\")\n",
    "oqt.quantize_static(\n",
    "        model_input=\"fp32_preproc.onnx\", \n",
    "        model_output=\"int8.onnx\", \n",
    "        calibration_data_reader=calib_reader,\n",
    "        quant_format=oqt.QuantFormat.QDQ,\n",
    "        weight_type=oqt.QuantType.QInt8,\n",
    "        per_channel=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329f7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
